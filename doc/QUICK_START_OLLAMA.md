# GuÃ­a RÃ¡pida: Ollama Local en IAExample

## âœ… ConfiguraciÃ³n Completada

El proyecto ha sido configurado para usar **Ollama** con el modelo **qwen2** de forma local.

## ğŸ“‹ QuÃ© Se Ha Modificado

### Archivos Modificados:
1. **src/main/resources/application.properties**
   - URL cambiada a `http://localhost:11434/api/chat`
   - Modelo configurado como `qwen2`
   - Timeout aumentado a 60 segundos
   - Eliminada la validaciÃ³n de API key

2. **src/main/java/com/example/iaexample/service/DeepseekService.java**
   - Removida validaciÃ³n de API key (no necesaria para Ollama)
   - Removido header Authorization (no necesario para Ollama)
   - Mensajes de error actualizados

3. **src/main/java/com/example/iaexample/config/DeepseekProperties.java**
   - AÃ±adida propiedad `stream` para soporte futuro

### Archivos Creados:
1. **OLLAMA_SETUP.md** - DocumentaciÃ³n completa de Ollama
2. **application.properties.example** - Configuraciones de ejemplo
3. **sh/test-ollama.sh** - Script de testing automatizado

### Archivos Actualizados:
1. **README.md** - AÃ±adida secciÃ³n de Ollama
2. **DEEPSEEK.md** - AÃ±adida nota sobre Ollama como alternativa

## ğŸš€ Inicio RÃ¡pido

### 1. Verificar Ollama
```bash
ollama list
```
DeberÃ­as ver `qwen2` en la lista.

### 2. Asegurar que Ollama estÃ¡ corriendo
```bash
systemctl status ollama
# o simplemente
curl http://localhost:11434/api/tags
```

### 3. Iniciar la aplicaciÃ³n
```bash
mvn spring-boot:run
```

### 4. Probar la integraciÃ³n
```bash
# OpciÃ³n 1: Script automÃ¡tico
./sh/test-ollama.sh

# OpciÃ³n 2: Manualmente
curl -X POST http://localhost:8080/api/ia/consultar \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Â¿CuÃ¡les son las asignaturas disponibles?"}'
```

## ğŸ”§ Cambiar de Modelo

Para usar otro modelo de Ollama:

1. Descargar el modelo:
```bash
ollama pull phi3
```

2. Editar `application.properties`:
```properties
deepseek.api.model=phi3
```

3. Reiniciar la aplicaciÃ³n

### Modelos Disponibles:
- `qwen2:1.5b` - Ultra ligero (~900MB) âš¡
- `phi3` - Eficiente (~2.3GB) ğŸ¯
- `gemma:2b` - Ligero (~1.4GB) ğŸ’
- `llama3.2` - VersÃ¡til (~2GB) ğŸ¦™
- `qwen2` - Default (~4.4GB) â­

## ğŸ“Š Rendimiento Esperado

En tu MacBook Pro 2015:
- **Latencia primera respuesta**: 3-8 segundos
- **Velocidad generaciÃ³n**: 5-10 tokens/segundo
- **Uso RAM**: 5-8GB (modelo + contexto)

## ğŸ› Troubleshooting

### Error "Connection refused"
```bash
# Iniciar Ollama
ollama serve
```

### Respuestas muy lentas
- Prueba con `qwen2:1.5b` o `gemma:2b`
- Reduce `max-tokens` en application.properties

### Modelo no encontrado
```bash
ollama pull qwen2
```

## ğŸ“š DocumentaciÃ³n Completa

- **[OLLAMA_SETUP.md](OLLAMA_SETUP.md)** - GuÃ­a detallada de instalaciÃ³n, configuraciÃ³n y optimizaciÃ³n
- **[README.md](../README.md)** - DocumentaciÃ³n general del proyecto
- **[DEEPSEEK.md](DEEPSEEK.md)** - Alternativa con Deepseek API

## ğŸ¯ PrÃ³ximos Pasos

1. **Experimentar con diferentes modelos** para encontrar el mejor balance
2. **Ajustar temperatura** segÃºn tus necesidades (0.0 = preciso, 1.0 = creativo)
3. **Optimizar max-tokens** para respuestas mÃ¡s rÃ¡pidas
4. **Implementar streaming** para respuestas en tiempo real
5. **AÃ±adir system prompts** personalizados para tu dominio

## âš¡ Ventajas de Ollama

âœ… **Gratis** - Sin costos de API
âœ… **Privado** - Datos procesados localmente
âœ… **Sin lÃ­mites** - Requests ilimitados
âœ… **Offline** - No requiere Internet
âœ… **RÃ¡pido** - Latencia mÃ­nima (localhost)
âœ… **Compatible** - API tipo OpenAI

Â¡Disfruta de tu IA local! ğŸš€
